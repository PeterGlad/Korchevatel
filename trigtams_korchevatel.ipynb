{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from random import uniform\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import io\n",
    "from transiltor import ScopusExtractor\n",
    "\n",
    "#r_alphabet = re.compile(u'[а-яА-Я0-9-]+|[.,:;?!]+')\n",
    "r_alphabet = re.compile(u'[a-zA-Z0-9-]+|[.,:;?!$]+')\n",
    "\n",
    "def gen_lines(corpus):\n",
    "    #data = open(corpus, encoding='utf-8')\n",
    "    #for line in data:\n",
    "    return corpus.lower()\n",
    "        #line.lower()\n",
    "def gen_tokens(lines):\n",
    "    #for line in lines:\n",
    "    for token in r_alphabet.findall(lines):\n",
    "        yield token\n",
    "\n",
    "def gen_trigrams(tokens):\n",
    "    t0, t1 = '$', '$'\n",
    "    for t2 in tokens:\n",
    "        yield t0, t1, t2\n",
    "        if t2 in '.!?':\n",
    "            yield t1, t2, '$'\n",
    "            yield t2, '$','$'\n",
    "            t0, t1 = '$', '$'\n",
    "        else:\n",
    "            t0, t1 = t1, t2\n",
    "\n",
    "def train_trigram(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "\n",
    "    bi, tri = defaultdict(lambda: 0.0), defaultdict(lambda: 0.0)\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        bi[t0, t1] += 1\n",
    "        tri[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}\n",
    "    for (t0, t1, t2), freq in tri.items():\n",
    "        if (t0, t1) in model:\n",
    "            model[t0, t1].append((t2, freq/bi[t0, t1]))\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, freq/bi[t0, t1])]\n",
    "    return model\n",
    "\n",
    "def generate_sentence(model):\n",
    "    phrase = ''\n",
    "    t0, t1 = '$', '$'\n",
    "    while 1:\n",
    "        t0, t1 = t1, unirand(model[t0, t1])\n",
    "        if t1 == '$': break\n",
    "        if t1 in ('.!?,;:') or t0 == '$':\n",
    "            phrase += t1\n",
    "        else:\n",
    "            phrase += ' ' + t1\n",
    "    return phrase.capitalize()\n",
    "\n",
    "def unirand(seq):\n",
    "    sum_, freq_ = 0, 0\n",
    "    for item, freq in seq:\n",
    "        sum_ += freq\n",
    "    rnd = uniform(0, sum_)\n",
    "    for token, freq in seq:\n",
    "        freq_ += freq\n",
    "        if rnd < freq_:\n",
    "            return token\n",
    "        \n",
    "def abstract_summator(path_to_json_file, input_name):\n",
    "    papers_json=[]\n",
    "    json_file = io.open('texts/users_articles_data_2.json',encoding='utf-8')\n",
    "    for line in json_file:\n",
    "        papers_json.append(json.loads(json_file.readline()))\n",
    "    abstract_sum=[]\n",
    "    extractor = ScopusExtractor('users_articles_data_2.json', d1=0.5, f1=0.5)\n",
    "    english_author_name = extractor.try_get_scopus_ids(input_name)\n",
    "    name = list(english_author_name[0])[0]\n",
    "    print(name)\n",
    "    for i in range(len(papers_json)):\n",
    "        if ('authorlist' in list(papers_json[i].keys())):\n",
    "            for names in papers_json[i]['authorlist']:\n",
    "                if (name in names.lower()):\n",
    "                    abstract_sum.append(papers_json[i]['abstract'])\n",
    "    abstract_sum=' '.join(abstract_sum)       \n",
    "    return abstract_sum\n",
    "        \n",
    "def korchevatel(path_to_json_file, input_name, number_of_sentences=2):\n",
    "    generated_text=[]\n",
    "    abstract_sum = abstract_summator(path_to_json_file, input_name)\n",
    "    model = train_trigram(abstract_sum)\n",
    "    for i in range(number_of_sentences):\n",
    "        generated_text.append(generate_sentence(model))\n",
    "    generated_text=' '.join(generated_text) \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guleva, v.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Current paper propose agent-based model of a timeline into branches. Any modifications in simulation parameters result in division of a timeline into branches.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_json_file = 'texts/users_articles_data_2.json'\n",
    "input_name = 'Гулева Вал'\n",
    "number_of_sentences = 3\n",
    "korchevatel(path_to_json_file, input_name, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
